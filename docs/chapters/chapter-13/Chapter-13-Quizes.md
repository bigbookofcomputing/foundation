
!!! note "Quiz"

    ??? question "1. The problem of solving for the unknown vector $\mathbf{x}$ in the equation $\mathbf{A}\mathbf{x} = \mathbf{b}$ is known as a:"
        - a) Eigenvalue problem
        - b) **System of linear equations**
        - c) Root-finding problem
        - d) Boundary value problem

        ??? info "See Answer"
            **b) System of linear equations**. This is the fundamental algebraic structure that many physics problems, once discretized, are transformed into.

    ??? question "2. What is the primary reason computational physicists avoid solving $\mathbf{A}\mathbf{x} = \mathbf{b}$ by calculating the inverse matrix, $\mathbf{x} = \mathbf{A}^{-1}\mathbf{b}$?"
        - a) The inverse matrix does not exist for most physical systems.
        - b) **Calculating the inverse is computationally expensive ($\mathcal{O}(N^3)$ with a large pre-factor) and numerically unstable.**
        - c) The inverse matrix is always sparse and difficult to store.
        - d) The solution vector $\mathbf{x}$ is not needed, only the inverse matrix.

        ??? info "See Answer"
            **b) Calculating the inverse is computationally expensive ($\mathcal{O}(N^3)$ with a large pre-factor) and numerically unstable.** Direct inversion is both slower and more prone to round-off error amplification than using a dedicated solver.

    ??? question "3. The direct solver method, LU Decomposition, factors the matrix $\mathbf{A}$ into what two types of matrices?"
        - a) A symmetric matrix and an anti-symmetric matrix.
        - b) A diagonal matrix and a sparse matrix.
        - c) **A lower triangular matrix ($\mathbf{L}$) and an upper triangular matrix ($\mathbf{U}$).**
        - d) An orthogonal matrix and a rotation matrix.

        ??? info "See Answer"
            **c) A lower triangular matrix ($\mathbf{L}$) and an upper triangular matrix ($\mathbf{U}$).** This factorization, $\mathbf{A} = \mathbf{L}\mathbf{U}$, is the core of modern direct solvers.

    ??? question "4. What is the main advantage of using LU Decomposition to solve a system where $\mathbf{A}$ is constant but $\mathbf{b}$ changes many times (like in the Crank-Nicolson method)?"
        - a) The factorization step is $\mathcal{O}(N^2)$.
        - b) **The expensive $\mathcal{O}(N^3)$ factorization is done only once, and each subsequent solution is found quickly in $\mathcal{O}(N^2)$ time using forward/backward substitution.**
        - c) It avoids the need for boundary conditions.
        - d) It is an iterative method that converges quickly.

        ??? info "See Answer"
            **b) The expensive $\mathcal{O}(N^3)$ factorization is done only once, and each subsequent solution is found quickly in $\mathcal{O}(N^2)$ time using forward/backward substitution.** This makes it extremely efficient for time-dependent problems with a constant system matrix.

    ??? question "5. The matrices generated by the Finite Difference Method (FDM) for 1D problems are typically:"
        - a) Dense and full
        - b) **Sparse and tridiagonal**
        - c) Diagonal
        - d) Random

        ??? info "See Answer"
            **b) Sparse and tridiagonal**. The FDM stencil only couples a point to its immediate neighbors, resulting in a matrix with non-zero elements only on the main, sub-, and super-diagonals.

    ??? question "6. What is the computational complexity of the Thomas Algorithm, which is specialized for solving tridiagonal systems?"
        - a) $\mathcal{O}(N^3)$
        - b) $\mathcal{O}(N^2)$
        - c) $\mathcal{O}(N \log N)$
        - d) **$\mathcal{O}(N)$**

        ??? info "See Answer"
            **d) $\mathcal{O}(N)$**. The Thomas Algorithm is a highly efficient linear-time solver that exploits the sparse, banded structure of tridiagonal matrices, making it the method of choice for 1D FDM problems.

    ??? question "7. For which type of problem are iterative solvers like Gauss-Seidel or Conjugate Gradient absolutely essential?"
        - a) Small, $3 \times 3$ systems.
        - b) Any system with a symmetric matrix $\mathbf{A}$.
        - c) **Extremely large, sparse systems, such as those from 2D or 3D FDM problems.**
        - d) Systems where the source vector $\mathbf{b}$ is zero.

        ??? info "See Answer"
            **c) Extremely large, sparse systems, such as those from 2D or 3D FDM problems.** For these systems, the $\mathcal{O}(N^3)$ cost and memory requirements of direct solvers are prohibitive, but iterative methods can find a solution efficiently by only operating on the non-zero elements.

    ??? question "8. Gaussian Elimination transforms the matrix $\mathbf{A}$ into an upper triangular matrix $\mathbf{U}$. The solution is then found by a process called:"
        - a) Forward substitution
        - b) **Back substitution**
        - c) Pivoting
        - d) Relaxation

        ??? info "See Answer"
            **b) Back substitution**. Once the system is in the form $\mathbf{U}\mathbf{x} = \mathbf{b}'$, the last unknown $x_N$ can be solved for directly, and the remaining unknowns are found by working backward up the rows.

    ??? question "9. In the context of solving for the voltages in a resistor network, the matrix $\mathbf{A}$ in the system $\mathbf{A}\mathbf{V} = \mathbf{I}$ is known as the:"
        - a) Resistance matrix
        - b) **Conductance matrix**
        - c) Voltage matrix
        - d) Current matrix

        ??? info "See Answer"
            **b) Conductance matrix**. The elements of the matrix $\mathbf{A}$ are derived from the conductances ($G = 1/R$) connecting the nodes of the circuit.

    ??? question "10. The process of swapping rows in Gaussian Elimination to avoid division by a small or zero pivot element is called:"
        - a) Factoring
        - b) Relaxation
        - c) **Pivoting**
        - d) Substitution

        ??? info "See Answer"
            **c) Pivoting**. Pivoting is crucial for the numerical stability of Gaussian elimination and LU decomposition, ensuring that round-off errors are not excessively amplified.

    ??? question "11. The Python code in Project 1 demonstrated that for a large tridiagonal system, the specialized `scipy.linalg.solve_banded` function was:"
        - a) Slightly slower than the general `lu_solve` function.
        - b) **Significantly faster than the general `lu_solve` function.**
        - c) Less accurate than the general `lu_solve` function.
        - d) Unable to solve the system.

        ??? info "See Answer"
            **b) Significantly faster than the general `lu_solve` function.** The output showed a speedup of over 70x, confirming the massive efficiency gain of using a specialized $\mathcal{O}(N)$ solver over a general $\mathcal{O}(N^3)$ solver for a sparse, banded matrix.

    ??? question "12. The Python code in Project 2 compared solving a system using `solve(A, b)` versus `inv(A) @ b`. What did the results show?"
        - a) The inverse method was faster and more accurate.
        - b) Both methods had identical speed and accuracy.
        - c) **The inverse method was slower and produced a solution with a larger numerical error.**
        - d) The `solve` method failed, while the inverse method succeeded.

        ??? info "See Answer"
            **c) The inverse method was slower and produced a solution with a larger numerical error.** This empirically validates the core lesson of the chapter: always use a dedicated solver instead of explicit matrix inversion.

    ??? question "13. The Conjugate Gradient (CG) method is a powerful iterative solver, but it is typically restricted to matrices $\mathbf{A}$ that are:"
        - a) Tridiagonal
        - b) **Symmetric and positive-definite**
        - c) Skew-symmetric
        - d) Invertible

        ??? info "See Answer"
            **b) Symmetric and positive-definite**. The mathematical guarantees of the CG method's convergence rely on these properties, which are fortunately common in matrices derived from physical problems.

    ??? question "14. After LU-decomposing a matrix $\mathbf{A}$ into $\mathbf{L}$ and $\mathbf{U}$, the system $\mathbf{A}\mathbf{x} = \mathbf{b}$ is solved in two steps. The first step is solving $\mathbf{L}\mathbf{y} = \mathbf{b}$ for the intermediate vector $\mathbf{y}$. This is done via:"
        - a) **Forward substitution**
        - b) Back substitution
        - c) Matrix inversion
        - d) An iterative guess

        ??? info "See Answer"
            **a) Forward substitution**. Because $\mathbf{L}$ is lower-triangular, the first unknown $y_1$ can be solved for directly, and the rest are found by working forward down the rows.

    ??? question "15. The transition from this chapter (solving $\mathbf{A}\mathbf{x} = \mathbf{b}$) to the next chapter (Chapter 14) involves moving from the 'driven' problem to the 'natural' or 'undriven' problem, which is known as the:"
        - a) Root-finding problem
        - b) **Eigenvalue problem ($\mathbf{A}\mathbf{x} = \lambda \mathbf{x}$)**
        - c) Optimization problem
        - d) Fourier analysis problem

        ??? info "See Answer"
            **b) Eigenvalue problem ($\mathbf{A}\mathbf{x} = \lambda \mathbf{x}$)**. This equation describes the natural modes and frequencies of a system (like in quantum mechanics or vibrations) when there is no external driving force $\mathbf{b}$.

    ??? question "16. What is the computational complexity of multiplying an $N \times N$ matrix by an $N \times 1$ vector?"
        - a) $\mathcal{O}(N^3)$
        - b) **$\mathcal{O}(N^2)$**
        - c) $\mathcal{O}(N)$
        - d) $\mathcal{O}(N \log N)$

        ??? info "See Answer"
            **b) $\mathcal{O}(N^2)$**. This is the cost of the final step in the matrix inversion method ($\mathbf{A}^{-1}\mathbf{b}$) and the cost of the forward/backward substitution steps in LU decomposition.

    ??? question "17. The relaxation methods (Jacobi, Gauss-Seidel) used in Chapter 10 to solve Laplace's Equation are examples of what type of solver?"
        - a) Direct solvers
        - b) **Iterative solvers**
        - c) Specialized solvers
        - d) Exact solvers

        ??? info "See Answer"
            **b) Iterative solvers**. Relaxation methods start with a guess and iteratively update the solution grid until it converges, which is the definition of an iterative solver.

    ??? question "18. If you have a $1000 \times 1000$ dense matrix $\mathbf{A}$, approximately how much slower is an $\mathcal{O}(N^3)$ algorithm compared to an $\mathcal{O}(N^2)$ algorithm?"
        - a) 10 times slower
        - b) 100 times slower
        - c) **1000 times slower**
        - d) They have the same speed.

        ??? info "See Answer"
            **c) 1000 times slower**. The ratio of the complexities is $N^3 / N^2 = N$. For $N=1000$, the cubic algorithm will be roughly 1000 times slower, highlighting the importance of algorithmic efficiency.

    ??? question "19. The `scipy.linalg.solve_banded` function is an implementation of what algorithm?"
        - a) A general LU decomposition
        - b) The Conjugate Gradient method
        - c) **The Thomas Algorithm (or a similar banded LU decomposition)**
        - d) The Jacobi method

        ??? info "See Answer"
            **c) The Thomas Algorithm (or a similar banded LU decomposition)**. This function is specifically designed to efficiently solve systems where the matrix is banded (e.g., tridiagonal, pentadiagonal).

    ??? question "20. Why is it stated that the FDM "magic trick" is converting calculus into linear algebra?"
        - a) Because calculus is not useful for physics.
        - b) **Because it replaces differential operators (like $\frac{d^2}{dx^2}$) with matrix operations, allowing problems in calculus to be solved using highly optimized linear algebra algorithms.**
        - c) Because linear algebra is easier to learn than calculus.
        - d) Because all physical laws are fundamentally linear.

        ??? info "See Answer"
            **b) Because it replaces differential operators (like $\frac{d^2}{dx^2}$) with matrix operations, allowing problems in calculus to be solved using highly optimized linear algebra algorithms.** This transformation is the cornerstone of many modern computational methods.

    ??? question "21. In the LU decomposition workflow for Crank-Nicolson, the $\mathcal{O}(N^3)$ factorization is performed:"
        - a) At every time step.
        - b) **Once, before the time-stepping loop begins.**
        - c) Once, after the time-stepping loop ends.
        - d) It is not needed for Crank-Nicolson.

        ??? info "See Answer"
            **b) Once, before the time-stepping loop begins.** Since the matrix $\mathbf{A}$ is constant in the standard Crank-Nicolson method, the expensive factorization is done only one time, making the subsequent time steps very fast.

    ??? question "22. A matrix is considered 'sparse' if:"
        - a) It has very small numerical values.
        - b) It is not invertible.
        - c) **Most of its elements are zero.**
        - d) All of its elements are zero.

        ??? info "See Answer"
            **c) Most of its elements are zero.** Sparsity is a key property that allows for the use of highly efficient storage formats and iterative solvers.

    ??? question "23. The final step of LU decomposition is solving $\mathbf{U}\mathbf{x} = \mathbf{y}$ for the solution $\mathbf{x}$. This is done via:"
        - a) Forward substitution
        - b) **Back substitution**
        - c) Another LU decomposition
        - d) Matrix inversion

        ??? info "See Answer"
            **b) Back substitution**. Since $\mathbf{U}$ is an upper-triangular matrix, this system is easily solved by starting from the last row and working backwards.

    ??? question "24. The core idea of iterative solvers is to:"
        - a) Find the exact solution in a single step.
        - b) **Start with a guess and repeatedly refine the solution until it converges to a desired tolerance.**
        - c) Calculate the inverse of the matrix piece by piece.
        - d) Use a random search to find the solution vector.

        ??? info "See Answer"
            **b) Start with a guess and repeatedly refine the solution until it converges to a desired tolerance.** This approach avoids the high cost of direct methods and is ideal for very large, sparse systems.

    ??? question "25. The problem of finding the natural modes of oscillation of a system is mathematically formulated as:"
        - a) A system of linear equations, $\mathbf{A}\mathbf{x} = \mathbf{b}$
        - b) A non-linear system, $f(\mathbf{x}) = 0$
        - c) **An eigenvalue problem, $\mathbf{A}\mathbf{x} = \lambda \mathbf{x}$**
        - d) An initial value problem, $\frac{d\mathbf{x}}{dt} = f(\mathbf{x}, t)$

        ??? info "See Answer"
            **c) An eigenvalue problem, $\mathbf{A}\mathbf{x} = \lambda \mathbf{x}$**. The eigenvalues ($\lambda$) correspond to the squared frequencies of the natural modes, and the eigenvectors ($\mathbf{x}$) describe the shapes of those modes.
